---
title: "Predicting Colon Cancer Metastasis with a Logstical Regression Model"
author: "Alistair Quinn"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Initial Questions

Can we predict which colon cancer patients will become metastatic, based on initial demographic information and clinical features of the cancer?

### Overview and Motivation

Metastatic cancer (spread of the tumour to other parts of the body) is associated with a much higher morbidity (death rate) than non-metastatic cancer.

It is valuable to predict early which patients are likely to be metastatic, so we can target more-aggressive chemotherapy and prevent disease progression, while sparing lower-risk patients the side-effects of highly toxic chemotherapy and radiotherapy.

In addition, performing full-body scans to exclude metastasis is very expensive, and the radiation involved can even increase a patient's risk of subsequent cancer. Thus we want to minimise unnecessary and potentially harmful investigations and treatments through effective risk stratification.

The aim of this project was to create a simple multivariate model to predict metastasis from basic predictors, such as age, tumour behaviour and grade.

### Data

As discussed earlier, the SEER dataset combines cancer registries covering approximately 34.6% of the US population, collecting data on demographics, tumour site, tumour morphology, stage and grade at diagnosis, treatment and follow-up. We reduced the initial dataset to patients with colon cancer between 2005 and 2015.

### Related Work

#### Prior Research - Computer Algorithms to Predict Life-threatening Medical Conditions

My contribution to this project was inspired by our class lectures and discussion on Machine Learning. It was also informed by my work as Adjunct Researcher at the Centre for Pattern Recognition and Data Analytics (PRaDA) at Deakin University in Victoria, Australia, with my principle investigator, Prof. Svetha Venkatesh.

The work of her team includes a landmark study which involved creating a machine learning algorithm that predicted risk of subsequent suicide after discharge, among patients who presented to the Emergency Department with suicide attempts. The model used demographic data and billing codes, was implemented in the Victorian healthcare system in Geelong, and exceeded prediction accuracy of clinicians. Accurately identifying high-risk patients can allow us to allocate limited resources (eg. admission to hospital) in the most beneficial way possible.

The strategy of using statistical methods to predict escalation in severity of patient outcomes has been applied to predicting the development of acute kidney injury among hospital in-patients (Google's DeepMind), as well as many other studies at a range of universities predicting escalation of infections (such as urinary tract infection or chest infection) to life-threatening sepsis, or predicting mortality from cancer.

#### Clinical Relevance

In addition informing resource allocation treatment approaches, prediction algorithms can assist doctors in clinical diagnosis. The process of clinical medical diagnosis involves identifying the correct disease causing a patient's symoptoms, but it also involves correcting identifying 'red flags', ie. signs and symptoms that could herald life-threatening conditions.

For example a child with fever, headache and mild cough / sneeze etc. will usually have a benign viral infection, but rarely may have a life-threatening case of bacterial meningitis. Correctly identifying these rare cases is exceptionally difficult to clinicians. Failing to diagnose them leads to tragic consequences, and overly defensive treatment can be expensive, traumatic to patients and can have their own risks. For example, analysing samples of cerebro-spinal fluid taking from the spine with needle in lumbar puncture can help clinicians to exclude meningitis, but the procedure is risky, distressing and painful.

#### Bayesian reasoning in medicine

In the era of evidence-based medicine, clinicians are focused on utilising rational approaches to diagnosis and management. ONe way that these algorithms can be utilised in clinical practice is in Bayesian reasoning. For example, a clinician may have a 'prior probability' of a certain life-threatening diagnosis, and subsequent tests (including computer algorithm prediction outcomes), can help him/her update on these priors towards a more informed 'posterior probability'. 

### Importing Data

```{r}
options(digits=3)
set.seed(1)
library(tidyverse)
library(dslabs)
ds_theme_set()

colon_cancer <- read.csv("colon_cancer.csv", stringsAsFactors = FALSE)
head(colon_cancer)
names(colon_cancer)

```
As shown above, our dataframe now has 357,223 patients, and has 34 columns of variables (including demographic and clinical information).

First we will create a new column of the dataframe for metastasis status (M_CLASS_BINARY), and assign a value of 0 for patients that are not metastatic, and a value of 1 for those who are.

M_CLASS is a set of categories referring to the extent of metastasis (or lack thereof). Please refer to our Cancer 2 Dataset Dictionary for further information.

Thus we will filter out all NAs or MX values - codes 99 and 88.
Keep M0 as no metastasis - code 00.
Group all subcategories of metastasis together - M1, M1a, M1b, M1c, M1 NOS - codes 10,11,12,13,19.
And assign them to 0 and 1 in M_CLASS_BINARY respectively.

```{r}
data <- colon_cancer %>%
  filter(colon_cancer$M_CLASS < 88) %>%
  mutate(M_CLASS_BINARY = ifelse(M_CLASS == "0", 0, 1))

names(data)

```

We will use three categories of features to predict the metastasis status - age categories, grades and cancer behaviour. I chose these three parameters as I feel they are clinically relevant parameters we would expect to be associated with metastasis (higher parameters for each being correlated with higher likelihood of metastasis). They are also all able to be assessed on the initial biopsy and consulation.

AGE is the patient's age at time of diagnosis.
BEHAVIOUR is a categorisation of cancer behaviour at time of diagnosis, ranging from from benign to malignant.
GRADE is a measure of tumour cell appearance - ie. whether the cancer appears to be aggressive based on appearance of its cells under the microscope.

First we will filter out the NAs for each category.

To remove NAs we filter by the following parameters:
AGE_CAT <99 (unknown age)
BEHAVIOUR <4 (all colon cancers in this timeframe should be coded as 0-3 and levels 4-6 are related to idiosyncrasies in combining earlier datasets). Note that when filtering here, the dataset did not decrease in size
GRADE <5 (1-4 are assocated with Grades 1 to IV, and above this represent NAs or refer to other types of cancer - eg. lymphoma and leukaemia)

```{r}

new_data <- data %>%
  filter(data$AGE_CAT < 99 & data$BEHAVIOR <4 & data$GRADE < 5)

head(new_data)
```
We have filtered the dataframe from 328610 datapoints to 274659, and are now able to perform a 'complete case analysis'.

Now we will measure the prevalence of metastasis in our dataset, because this has implications for the machine learning approach we deploy (unbalanced datasets may introduce bias and inaccurate results for either level of the binary variable we are classifying).

```{r}
prevalence <- sum(new_data$M_CLASS_BINARY == "1") / length(new_data$M_CLASS_BINARY)
prevalence
```
Thus the prevalence of colon cancer in this dataset is 17.1%. In an ideal analysis, we will use sampling methods to create more balanced dataset, however this is outside of the scope of this project.

Now we will create vectors with our 4 variables of interest. First we will explore the range of colon cancer cases.

```{r}
new_data$AGE[which.min(new_data$AGE)]
new_data$AGE[which.max(new_data$AGE)]
```

The age at diagnosis of colon cancer in the dataset ranges from 6 years (sadly) to 110 years.

Now we will visualise the counts of age at time of first diagnosis across all colon cancers.

```{r}
ages <- ggplot(new_data, aes(x=AGE)) + 
  geom_histogram(binwidth = 1, color = "black", fill = "lightskyblue2") +
  labs(title="Histogram of Colon Cancer Counts by Age") +
  labs(x="Age", y="Count")

ages
```
The age distribution is left-skewed, with some outlier counts, likely representing random variation.

Let's compare the distribution of colon cancer cases with metastasis against those without, using a smoothed density plot (stratified by metastasis status).

```{r}

new_data %>% ggplot(aes(x=AGE, fill=as.factor(M_CLASS_BINARY))) +
  geom_density(alpha = 0.2, bw = 3) +
  labs(title="Smoothed Density Plot of Colon Cancer Cases Stratified by Metastasis") +
  labs(x="Age", y="Proportion of Counts at Each Age") +
  scale_fill_discrete(name = "Metastasis Status")

```

Interestingly, the metastatic group appears to be about 3-5 years younger than the non-metastatic group. This is not expected, as we might hypothesise that patients who are older at time of diagnosis are more likely to be metastatic (as more time has elapsed for disease progression before diagnosis). On the other hand, younger people may be at higher risk of more aggressive cancers. In any case, age may be a good feature for prediction as there is a difference between the two outcome groups.

Next, let's show a two-way table comparing tumour behaviour categories against metastasis status.

```{r}
table(new_data$BEHAVIOR, new_data$M_CLASS_BINARY)
prop.table(table(new_data$BEHAVIOR, new_data$M_CLASS_BINARY), margin=2)
```
Columns are 0 and 1 (for no metastasis and metastasis respectively).

The rows can range from 0 to 3, but note that all patients in the dataset were either 2 (in situ), or 3 (malignant), with all metastatic patients malignant. This is as we would expect, because patients who are metastatic are more likely to have more malignant (aggressive) disease. 

```{r}
table(new_data$GRADE, new_data$M_CLASS_BINARY)
prop.table(table(new_data$GRADE, new_data$M_CLASS_BINARY), margin=2)

# install.packages("RColorBrewer")
# library(RColorBrewer)

barplot(prop.table(table(new_data$GRADE, new_data$M_CLASS_BINARY), margin=2),
        main = "Proportion of Cases at each Grade in Metastasis VS Non-Metastasis",
        xlab = "Metastasis Status",
        ylab = "Proportion",)

```

Darker shades of grey at the bottom of the graph correspond to lower cancer grades (1,2,3,4). The plot shows that non-metastatic cases (0) have a higher proportion of lower grades (1 and 2) compared to the metastatic cases (1). This difference should make grade an appropriate feature to predict metastasis.

Now we will split the data into even training and test sets.

```{r}
library(caret)
set.seed(1)
train_index <- createDataPartition(new_data$M_CLASS_BINARY, times = 1, p = 0.5, list = FALSE)
```

Use this index to define training and test sets:

```{r}
train_set <- new_data[train_index, ]
test_set <- new_data[-train_index, ]

head(train_set)
head(test_set)
```
Note that train_set has 137,330 rows and test_set has 137,329 rows.

We will 

```{r}
prevalence_train <- sum(train_set$M_CLASS_BINARY == "1") / length(train_set$M_CLASS_BINARY)
prevalence_train

prevalence_test <- sum(test_set$M_CLASS_BINARY == "1") / length(test_set$M_CLASS_BINARY)
prevalence_test

```
Note that the prevalence of metastasis in the training and test sets is similar (about 17%).

Next we will fit the logistic regression model for age, using the function glm (generalized linear model). Because our outcome variable of metastasis is a dichotomous / binary variable (0 or 1), we will specify binomial in the glm function.

```{r}
glm_fit_age <- glm(M_CLASS_BINARY ~ AGE, data=train_set, family = "binomial")
summary(glm_fit_age)
```
Next we will create generalised linear models for tumour behaviour and grade.

```{r}
glm_fit_behaviour <- glm(M_CLASS_BINARY ~ BEHAVIOR, data=train_set, family = "binomial")
summary(glm_fit_behaviour)

glm_fit_grade <- glm(M_CLASS_BINARY ~ GRADE, data=train_set, family = "binomial")
summary(glm_fit_grade)

```

Now we will calculate the confusion matrix, to test the performance of the logistic regression model for age.

```{r}

p_hat_age <- predict(glm_fit_age, newdata = test_set, type="response")
y_hat_age <- ifelse(p_hat_age > 0.5, 1, 0)
confusionMatrix(data = as.factor(y_hat_age), reference = as.factor(test_set$M_CLASS_BINARY))

```
As seen above, the first iteration of the model appears to have an accuracy of 0.83, however the sensitivity is 1 and the specificity is 0.

This is because the algorithm is classifying 100% of patients as metastatic. Thus all true positives are correctly predicted as positive (hence the sensitivity of 100%), and all true negatives are incorrectly predicted positive as well (hence the specificity of 0). So while the accuracy is technically high, the algorithm is not very useful, as it is cheating, due to our uneven groups (as noted above, the prevalence of metastasis is 17%).

We can adjust the p_hat_age cutoff to be greater than 0.17 to reflect this imbalance between levels of our metastasis outcome.

```{r}
y_hat_age <- ifelse(p_hat_age > 0.17, 1, 0)
confusionMatrix(data = as.factor(y_hat_age), reference = as.factor(test_set$M_CLASS_BINARY))

```
Now we have an accuracy of 0.545, which is slightly better than a coin-toss. Our sensitivity and specificity are now 0.541 and 0.564, showing that the algorithm is now predicting a reasonable prorpotion of the true positives as positive and true negatives as negative.

Next we will assess our algorithms for tumour behaviour.

```{r}
p_hat_behaviour <- predict(glm_fit_behaviour, newdata = test_set, type="response")
y_hat_behaviour <- ifelse(p_hat_behaviour > 0.17, 1, 0)
confusionMatrix(data = as.factor(y_hat_behaviour), reference = as.factor(test_set$M_CLASS_BINARY))

```

The accuracy for the univariate model of grade is unexpectedly low - 18.1% and the sensitivity is 0.0133 and specificity is 1. This likely reflects the fact that there was a zero count for a grade of 2 in the metastasis set (ie. all metastatic patients had a grade of 3). For this reason, we will exclude tumour behaviour from our final analysis.

Next and assessing the algorithm for tumour grade:

```{r}
p_hat_grade <- predict(glm_fit_grade, newdata = test_set, type="response")
y_hat_grade <- ifelse(p_hat_grade > 0.17, 1, 0)
confusionMatrix(data = as.factor(y_hat_grade), reference = as.factor(test_set$M_CLASS_BINARY))

```

The grade algorithm has an accuracy of 0.734, which is quite high. It has a high sensitivity (0.82), but a low specificity (0.315). This means that a high proportion of the truly positive cases are correctly identified as positive (sensitivity), but that a low proportion of the truly negative cases are correctly identified as negative. Higher sensitivity makes the test reasonable for use in screening for metastasis, as we don't want to miss any positive cases.

Next we will create an overall generalised linear model that includes age and grade, but excludes behaviour as it was a poor predictor in isolation.

```{r}
glm_fit_overall <- glm(M_CLASS_BINARY ~ AGE + GRADE, data=train_set, family = "binomial")
summary(glm_fit_overall)
```

And assess the confusion matrix:

```{r}
p_hat_overall <- predict(glm_fit_overall, newdata = test_set, type="response")
y_hat_overall <- ifelse(p_hat_overall > 0.17, 1, 0)
confusionMatrix(data = as.factor(y_hat_overall), reference = as.factor(test_set$M_CLASS_BINARY))

```

The overall algorithm has an accuracy of 59.3%, a sensitivty of 59.6% and a specificity of 57.7%. Thus it is marginally better at prediction than a coin toss.

Next we can reassess our algorithms, using the F1-score (the harmonic mean of precision and recall) instead:

```{r}

y_hat_age <- ifelse(p_hat_age > 0.17, 1, 0)
F_meas(data = as.factor(y_hat_age), reference = as.factor(test_set$M_CLASS_BINARY))

y_hat_behaviour <- ifelse(p_hat_behaviour > 0.17, 1, 0)
F_meas(data = as.factor(y_hat_behaviour), reference = as.factor(test_set$M_CLASS_BINARY))

y_hat_grade <- ifelse(p_hat_grade > 0.17, 1, 0)
F_meas(data = as.factor(y_hat_grade), reference = as.factor(test_set$M_CLASS_BINARY))

y_hat_overall <- ifelse(p_hat_overall > 0.17, 1, 0)
F_meas(data = as.factor(y_hat_overall), reference = as.factor(test_set$M_CLASS_BINARY))

```
With the exclusion of behaviour, the other univariate algorithms had reasonable F1 scores (0.663 and 0.837), and the F1 score for the overall multivariate algorithm (combining age and grade) is 0.708.

### Final Analysis

Ultimately, we created a logistic regression multivariate algorithm that combined age and grade on initial diagnosis of colon cancer to predict metastasis with better-than-chance accuracy (0.593). Next steps would be to add further features, with the aim of improving performance, and then testing it on a new dataset to ensure we didn't overfit the model.